{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression w/ Early Stopping \n",
    "### (Batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "# list(iris.keys()) (legend)\n",
    "\n",
    "X = iris['data'][:, (2, 3)] # 150 x 2\n",
    "X = np.c_[np.ones([len(X), 1]), X] # add 1's column basically (150 x 3)\n",
    "y = iris['target'] # 150 x 1\n",
    "\n",
    "class SoftmaxClassifier():\n",
    "    def __init__(self, X, y): \n",
    "        self.X         = X\n",
    "        self.y         = y\n",
    "        self.m         = X.shape[0]\n",
    "        self.n         = X.shape[1]\n",
    "        self.n_inputs  = X.shape[1]        # number of columns\n",
    "        self.n_outputs = len(np.unique(y)) # creates an array of unique classes and stores the len of that array\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test  = None\n",
    "        self.y_test  = None\n",
    "        self.X_valid = None\n",
    "        self.y_valid = None\n",
    "        \n",
    "        self.train_ratio      = 0.6\n",
    "        self.test_ratio       = 0.2\n",
    "        self.validation_ratio = 0.2\n",
    "        \n",
    "        self.train_test_split() # create the 3 sets before any other manipulation \n",
    "        \n",
    "        return\n",
    "        \n",
    "    def encode_classes(self, y):\n",
    "        y_dims                                  = y.shape\n",
    "        n_classes                               = y.max() + 1\n",
    "        one_hot_vector                          = np.zeros((y_dims[0], n_classes)) # introduce int or TUPLE of ints\n",
    "        one_hot_vector[np.arange(y_dims[0]), y] = 1 # arange creates an array with the size of y_train (30) in this case \n",
    "            \n",
    "        return one_hot_vector \n",
    "        \n",
    "    def train_test_split(self): \n",
    "        total_size      = self.m\n",
    "        test_size       = int(total_size * self.test_ratio)\n",
    "        validation_size = int(total_size * self.validation_ratio)\n",
    "        train_size      = total_size - test_size - validation_size\n",
    "        random_indices  = np.random.permutation(total_size) # creates shuffled array in that range\n",
    "        \n",
    "        self.X_train = self.X[random_indices[:train_size]]\n",
    "        self.y_train = self.y[random_indices[:train_size]]\n",
    "        self.X_test  = self.X[random_indices[train_size:(train_size + test_size)]]\n",
    "        self.y_test  = self.y[random_indices[train_size:(train_size + test_size)]]\n",
    "        self.X_valid = self.X[random_indices[(train_size + test_size):(train_size + test_size + validation_size)]]\n",
    "        self.y_valid = self.y[random_indices[(train_size + test_size):(train_size + test_size + validation_size)]]\n",
    "        \n",
    "        self.y_train = self.encode_classes(self.y_train)\n",
    "        \n",
    "        return \n",
    "        \n",
    "    def calculate_softmax_score(self, X, theta): \n",
    "        return X.dot(theta)\n",
    "    \n",
    "    def calculate_softmax_function(self, logits):\n",
    "        numerator   = np.exp(logits)\n",
    "        denominator = np.sum(numerator, axis=1, keepdims=True) # keepdims keep the original dimension of the matrix\n",
    "        \n",
    "        return numerator / denominator\n",
    "\n",
    "    def train_model(self):\n",
    "        eta          = 0.01\n",
    "        n_iterations = 10000 \n",
    "        alpha        = 0.1 # regularization \n",
    "        epsilon      = 1e-6\n",
    "        best_loss    = np.inf # initialize at infinity \n",
    "        theta        = np.random.rand(self.n_inputs, self.n_outputs) # 3 x 3  \n",
    "\n",
    "        for iteration in range(n_iterations):\n",
    "            # normal training w/ gradient descent\n",
    "            logits        = self.calculate_softmax_score(self.X_train, theta)\n",
    "            y_proba       = self.calculate_softmax_function(logits)    \n",
    "            gradients     = (1 / self.m) * (self.X_train.T.dot(y_proba - self.y_train)) + np.r_[np.zeros([1, self.n_outputs]), alpha * theta[1:]]\n",
    "            theta         = theta - (eta * gradients) # new theta value\n",
    "            \n",
    "            # manipulation of validation set for early stopping\n",
    "            logits_valid    = self.calculate_softmax_score(self.X_valid, theta)\n",
    "            y_proba_valid   = self.calculate_softmax_function(logits_valid)\n",
    "            y_valid_encode  = self.encode_classes(self.y_valid)   \n",
    "            cross_entropy   = -np.mean(np.sum(y_valid_encode * np.log(y_proba_valid + epsilon), axis=1))\n",
    "            loss_score      = cross_entropy + (alpha * ((1/2) * np.sum(np.square(theta[1:]))))\n",
    "        \n",
    "            if loss_score < best_loss: \n",
    "                best_loss = loss_score\n",
    "            else: \n",
    "                break # early stopping\n",
    "            \n",
    "        return theta\n",
    "    \n",
    "    def valid_predict(self):\n",
    "        theta           = self.train_model()\n",
    "        logits_valid    = self.calculate_softmax_score(self.X_valid, theta)\n",
    "        y_proba_valid   = self.calculate_softmax_function(logits_valid)\n",
    "        y_predict_valid = np.argmax(y_proba_valid, axis=1)\n",
    "        accuracy        = np.mean(y_predict_valid == self.y_valid)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def test_predict(self): \n",
    "        theta          = self.train_model()\n",
    "        logits_test    = self.calculate_softmax_score(self.X_test, theta)\n",
    "        y_proba_test   = self.calculate_softmax_function(logits_test)\n",
    "        y_predict_test = np.argmax(y_proba_test, axis=1)\n",
    "        accuracy       = np.mean(y_predict_test == self.y_test)\n",
    "        \n",
    "        return accuracy\n",
    "        \n",
    "softmax_classifier_iris = SoftmaxClassifier(X, y)\n",
    "softmax_classifier_iris.test_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
